<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="no-js ie6"><![endif]-->
<!--[if IE 7 ]><html class="no-js ie7"><![endif]-->
<!--[if IE 8 ]><html class="no-js ie8"><![endif]-->
<!--[if IE 9 ]><html class="no-js ie9"><![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    
      
        <title>Q-Learning Models - RL Helicopter Game</title>
      
      
      
      
        <meta name="author" content="Dan Dixey">
      
    
    <meta property="og:url" content="None">
    <meta property="og:title" content="RL Helicopter Game">
    <meta property="og:image" content="None/../">
    <meta name="apple-mobile-web-app-title" content="RL Helicopter Game">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
    
    <link rel="shortcut icon" type="image/x-icon" href="../None">
    <link rel="icon" type="image/x-icon" href="../None">
    <style>
      @font-face {
      	font-family: 'Icon';
      	src: url('../assets/fonts/icon.eot?52m981');
      	src: url('../assets/fonts/icon.eot?#iefix52m981')
               format('embedded-opentype'),
      		   url('../assets/fonts/icon.woff?52m981')
               format('woff'),
      		   url('../assets/fonts/icon.ttf?52m981')
               format('truetype'),
      		   url('../assets/fonts/icon.svg?52m981#icon')
               format('svg');
      	font-weight: normal;
      	font-style: normal;
      }
    </style>
    <link rel="stylesheet" href="../assets/stylesheets/application-54f87043f3.css">
    
      <link rel="stylesheet" href="../assets/stylesheets/palettes-05ab2406df.css">
    
    
      
      
      
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,700|Roboto+Mono">
      <style>
        body, input {
          font-family: 'Roboto', Helvetica, Arial, sans-serif;
        }
        pre, code {
          font-family: 'Roboto Mono', 'Courier New', 'Courier', monospace;
        }
      </style>
    
    
    <script src="../assets/javascripts/modernizr-4ab42b99fd.js"></script>
    
  </head>
  
  
  
  <body class="palette-primary-indigo palette-accent-light-blue">
    
      
      
    
    <div class="backdrop">
      <div class="backdrop-paper"></div>
    </div>
    <input class="toggle" type="checkbox" id="toggle-drawer">
    <input class="toggle" type="checkbox" id="toggle-search">
    <label class="toggle-button overlay" for="toggle-drawer"></label>
    <header class="header">
      <nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        
          <span class="path">
            
              
                Project <i class="icon icon-link"></i>
              
            
          </span>
        
        Q-Learning Models
      </div>
    </div>
    
    
    <div class="button button-search" role="button" aria-label="Search">
      <label class="toggle-button icon icon-search" title="Search" for="toggle-search"></label>
    </div>
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
    </header>
    <main class="main">
      
      <div class="drawer">
        <nav aria-label="Navigation">
  
  <a href="https://github.com/dandxy89/rf_helicopter" class="project">
    <div class="banner">
      
      <div class="name">
        <strong>
          RL Helicopter Game
          <span class="version">
            
          </span>
        </strong>
        
          <br>
          dandxy89/rf_helicopter
        
      </div>
    </div>
  </a>
  <div class="scrollable">
    <div class="wrapper">
      
        <ul class="repo">
          <li class="repo-download">
            
            <a href="https://github.com/dandxy89/rf_helicopter/archive/master.zip" target="_blank" title="Download" data-action="download">
              <i class="icon icon-download"></i> Download
            </a>
          </li>
          <li class="repo-stars">
            <a href="https://github.com/dandxy89/rf_helicopter/stargazers" target="_blank" title="Stargazers" data-action="star">
              <i class="icon icon-star"></i> Stars
              <span class="count">&ndash;</span>
            </a>
          </li>
        </ul>
        <hr>
      
      <div class="toc">
        <ul>
          
            
  <li>
    <a class="" title="Overview" href="..">
      Overview
    </a>
    
  </li>

          
            
  <li>
    <span class="section">Project</span>
    <ul>
      
        
  <li>
    <a class="" title="Detailed Overview" href="../project_detail/">
      Detailed Overview
    </a>
    
  </li>

      
        
  <li>
    <a class="" title="Implementation" href="../implementation/">
      Implementation
    </a>
    
  </li>

      
        
  <li>
    <a class="current" title="Q-Learning Models" href="./">
      Q-Learning Models
    </a>
    
      
      
        <ul>
          
            <li class="anchor">
              <a title="Models Implemented:" href="#models-implemented">
                Models Implemented:
              </a>
            </li>
          
            <li class="anchor">
              <a title="Overview of Q-Learning" href="#overview-of-q-learning">
                Overview of Q-Learning
              </a>
            </li>
          
            <li class="anchor">
              <a title="Motivation for DQN" href="#motivation-for-dqn">
                Motivation for DQN
              </a>
            </li>
          
            <li class="anchor">
              <a title="Implementing the DQN" href="#implementing-the-dqn">
                Implementing the DQN
              </a>
            </li>
          
            <li class="anchor">
              <a title="Expectations" href="#expectations">
                Expectations
              </a>
            </li>
          
        </ul>
      
    
  </li>

      
        
  <li>
    <a class="" title="Release Notes" href="../release_notes/">
      Release Notes
    </a>
    
  </li>

      
        
  <li>
    <a class="" title="Video Demo" href="../videos/">
      Video Demo
    </a>
    
  </li>

      
        
  <li>
    <a class="" title="References" href="../references/">
      References
    </a>
    
  </li>

      
    </ul>
  </li>

          
        </ul>
        
      </div>
    </div>
  </div>
</nav>
      </div>
      <article class="article">
        <div class="wrapper">
          
            <h1>Q-Learning Models</h1>
          
          <h2 id="models-implemented">Models Implemented:<a class="headerlink" href="#models-implemented" title="Permanent link">#</a></h2>
<ol>
<li>e-Greedy Q Learning (1)</li>
<li>e-Greedy Q-Learning with e-decay (2)</li>
<li>Deep Q-Network (<em>DQN</em>)(3)</li>
</ol>
<h2 id="overview-of-q-learning">Overview of Q-Learning<a class="headerlink" href="#overview-of-q-learning" title="Permanent link">#</a></h2>
<p>All the models discussed in the coursework will use an online update policy strategy – an epsilon-greedy which intends to ensure adequate exploration of all the state space. Rummery and Niranjan (Rmmery et al., 1994) [9] provides an example of setting in which similar procedure are adopted. To exploit differences in the result of this self-driving helicopter simulation, we will compare three methods in which a different policy has been applied to select the best actions.</p>
<p>Q-learning uses temporal differences to estimate the value of Q<em>(s,a). In Q-learning, the agent maintains a table of Q[S,A], where S is the set of states and A is the set of actions. Q[s,a] represents its current estimate of Q</em>(s,a).</p>
<p>The Q-Learning variables are represented below:</p>
<ul>
<li>Q(s, a) = Q value of a given state and action</li>
<li>a = Action</li>
<li>s = State</li>
<li>r = Reward</li>
<li>R = Maximum reward for action a</li>
</ul>
<p>Where Q-Learning can be represented by:</p>
<pre><code>Q[s,a] ←Q[s,a] + α(r+ γmaxa' Q[s',a'] - Q[s,a])
</code></pre>
<h2 id="motivation-for-dqn">Motivation for DQN<a class="headerlink" href="#motivation-for-dqn" title="Permanent link">#</a></h2>
<p>Last year, Deep Q Networks (DQN) were brought to the attention of many researchers when Deepmind released a paper demonstrating the network's capability at playing Atari games.</p>
<p>The research featured in the Nature publication and showed that their implementation had overcome the issues that had typically been challenged when using a Neural Network as a function approximation for the Q values. Summarized in the table below, from the paper Playing Atari with Deep Reinforcement Learning (Deepmind, 2015) [16], the issue is discussed as well as the techniques that have been used to overcome these problems.</p>
<table>
<thead>
<tr>
<th>Issues</th>
<th>Techniques</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stability Issues</td>
<td>Reward Clipping</td>
</tr>
<tr>
<td>Distribution of the data can change quickly</td>
<td>Error Clipping (Truncation)</td>
</tr>
</tbody>
</table>
<p>The issues outlined in the table were implemented and were shown to have a real impact on the capability of the approximation. The analysis indicated that normalizing the range of the reward to a finite range helped to support the issue of dealing with large Q-values and their respective gradients – one negative of this approach was that the model may find it harder to differentiate the difference between small and large rewards due to the normalization. The second technique that was introduced was error clipping into the model – this is a frequently used method to deal with the potential of exploding gradients. The pseudocode below is a high-level description of the methods that were implemented in the Deepmind paper and in our model.</p>
<h2 id="implementing-the-dqn">Implementing the DQN<a class="headerlink" href="#implementing-the-dqn" title="Permanent link">#</a></h2>
<p>The sequential nature of our problem means that it could potentially benefit from using a Recurrent Neural Network either as primary function approximator or as additional layers in the network, as in Mnih (Mnih et al., 2015) [15]. The original paper described that a Convolutional Neural Network (CNN) was used to “watch” the replays of the game. In our implementation of the Deep Q-Network, we used a CNN that outputs to a Long Short-Term Memory (LSTM) layer and then finally into a linear output layer providing the Q-Values from the model. The key distinction to the original paper was that an LSTM layer was used where it has been demonstrated in many papers previously that an RNN is accomplished at capturing temporal patterns in sequential data. Since the goal of Q-Learning is to learn good policies for sequential decision making it, therefore, seemed appropriate to include this layer type.</p>
<p>The implementation initially developed made use of similar/simpler architecture to that proposed in previous work. This architecture that was implemented consisted of an Embedding Layer, which mapped the integer values (state) to a one-dimensional vector which were then passed into a Convolutional layer, followed by a Max-Pooling layer and lastly into two Dense layers (one with a Relu activation and the last a Softmax function). The intention was to replace the Dense layers with Recurrent layers, however, throughout testing, it was found that stability and the time to experiment with varying factors meant that the desire to move to the potentially enhanced version could not be achieved within the time frame. One feature that was implemented was that the model could record and store instances of the transitions and experience replay. This idea was used in one the most successful use cases of Neural Networks in Reinforcement Learning; this model was called TD-Gammon.</p>
<p>The model was developed in Keras, a Deep Learning library for Python, the primary reason for this is that it provides a very easy layer of abstraction on top of Theano or Tensorflow.</p>
<p>The diagram below captures the architecture used for developing the model.</p>
<p><em>placeholder: field of view diagram</em></p>
<h2 id="expectations">Expectations<a class="headerlink" href="#expectations" title="Permanent link">#</a></h2>
<p>For both all of the Q-Learning variants it is expected that:</p>
<ol>
<li>the model will probably go back in forth between preferring different actions over others during the initialisation<ol>
<li>finding an appropriate learning rate and Gammma value for will be crucial</li>
<li>ensuring that the terminal state can be reached otherwise not enough learning can be stimulated</li>
</ol>
</li>
<li>DQN learning models will be sufficiently more difficult to training<ol>
<li>tuning of both the model and q-learning parameters will be critical to ensure the model converges</li>
</ol>
</li>
</ol>
          <aside class="copyright" role="note">
            
              Copyright (c) 2016 Dan Dixey &ndash;
            
            Documentation built with
            <a href="http://www.mkdocs.org" target="_blank">MkDocs</a>
            using the
            <a href="http://squidfunk.github.io/mkdocs-material/" target="_blank">
              Material
            </a>
            theme.
          </aside>
          
            <footer class="footer">
              
  <nav class="pagination" aria-label="Footer">
    <div class="previous">
      
        <a href="../implementation/" title="Implementation">
          <span class="direction">
            Previous
          </span>
          <div class="page">
            <div class="button button-previous" role="button" aria-label="Previous">
              <i class="icon icon-back"></i>
            </div>
            <div class="stretch">
              <div class="title">
                Implementation
              </div>
            </div>
          </div>
        </a>
      
    </div>
    <div class="next">
      
        <a href="../release_notes/" title="Release Notes">
          <span class="direction">
            Next
          </span>
          <div class="page">
            <div class="stretch">
              <div class="title">
                Release Notes
              </div>
            </div>
            <div class="button button-next" role="button" aria-label="Next">
              <i class="icon icon-forward"></i>
            </div>
          </div>
        </a>
      
    </div>
  </nav>

            </footer>
          
        </div>
      </article>
      <div class="results" role="status" aria-live="polite">
        <div class="scrollable">
          <div class="wrapper">
            <div class="meta"></div>
            <div class="list"></div>
          </div>
        </div>
      </div>
    </main>
    <script>
      var base_url = '..';
      var repo_id  = 'dandxy89/rf_helicopter';
    </script>
    <script src="../assets/javascripts/application-997097ee0c.js"></script>
    
    
  </body>
</html>