{
    "docs": [
        {
            "location": "/", 
            "text": "Project Objective:\n#\n\n\nThe purpose of this analysis is to provide the reader a concrete example of a classical Reinforcement Learning application.\n\n\nThe intention is to create a Reinforcement Learning algorithm to learn to play and complete the track, similar to the \nHelicopter Game\n. The domain of implementation will be the contest of a randomly generated environment according to a particular function, and the learning task will provide the Agent a set of information enabling him to survive in the current environment for a longer duration of time.\n\n\nThe environment will provide to the Agent the possibility to move in five possible actions (directions), and the survival function will be defined as that function allowing the Agent to avoid obstacles generated in the environment.\n\n\nProposed algorithms for this task are all online policy iterations models with e-greedy updates policies: Q-Learning, Q-Learning with Epsilon Decay, and Deep Q-Learning Networks (DQN). A comparison of different parameters will be performed and evaluated.\n\n\nCity University\n#\n\n\nThis project was completed as part of a University course work for which a \nFirst Class\n award was received.\n\n\n\n\nNote\n\n\nProject Structure:\n\n\n\n\n\n\nrf_helicoper/\n\n\n/Model\n : Contains all the Scripts create tracks, generate plots and Agent model types\n\n\n/Results\n : Contains all the saved memories and plots generated\n\n\n/Tests\n : Started working on py.tests to ensure code quality\n\n\n/docs\n : MKDocs documentation", 
            "title": "Overview"
        }, 
        {
            "location": "/#project-objective", 
            "text": "The purpose of this analysis is to provide the reader a concrete example of a classical Reinforcement Learning application.  The intention is to create a Reinforcement Learning algorithm to learn to play and complete the track, similar to the  Helicopter Game . The domain of implementation will be the contest of a randomly generated environment according to a particular function, and the learning task will provide the Agent a set of information enabling him to survive in the current environment for a longer duration of time.  The environment will provide to the Agent the possibility to move in five possible actions (directions), and the survival function will be defined as that function allowing the Agent to avoid obstacles generated in the environment.  Proposed algorithms for this task are all online policy iterations models with e-greedy updates policies: Q-Learning, Q-Learning with Epsilon Decay, and Deep Q-Learning Networks (DQN). A comparison of different parameters will be performed and evaluated.", 
            "title": "Project Objective:"
        }, 
        {
            "location": "/#city-university", 
            "text": "This project was completed as part of a University course work for which a  First Class  award was received.   Note  Project Structure:    rf_helicoper/  /Model  : Contains all the Scripts create tracks, generate plots and Agent model types  /Results  : Contains all the saved memories and plots generated  /Tests  : Started working on py.tests to ensure code quality  /docs  : MKDocs documentation", 
            "title": "City University"
        }, 
        {
            "location": "/project_detail/", 
            "text": "Helicopter_Game\n#\n\n\nThe purpose of this analysis is to provide the reader a concrete example of a classical Reinforcement Learning application.\n\n\nThe intention is to create a Reinforcement Learning algorithm to learn to play and complete the track, similar to the \nHelicopter Game\n. The domain of implementation will be the contest of a randomly generated environment according to a particular function, and the learning task will provide the Agent a set of information enabling him to survive in the current environment for a longer duration of time.\n\n\nThe environment will provide to the Agent the possibility to move in five possible actions (directions), and the survival function will be defined as that function allowing the Agent to avoid obstacles generated in the environment.\n\n\nProposed algorithms for this task are all online policy iterations models with e-greedy updates policies: Q-Learning, Q-Learning with Epsilon Decay, and Deep Q-Learning Networks (DQN). A comparison of different parameters will be performed and evaluated.\n\n\nRepresentation\n#\n\n\nThe helicopter task can be summarized as a survival task - the longer it travels along the path, the better its performance. However, the maximum reward is gained only by reaching the end of the track. The States following are represented by the different gradient colours in the part of the track which is not an obstacle, and the wind function guarantees a state which ranges from 0 to 6 according to which the helicopter is taking an action. Obstacles are initially set to be -10 and final reward to 100. The representation of the best route is the one by which the helicopter navigates the wind effectively and reaches the end of the track without any interruptions.\n\n\nThere are a number of possible actions it can take are either up, down, or continue laterally; different index increments dictate different actions. This index can range from 1 to 5, where an index of 1 would attempt to move the helicopter down by two, and an index of 5 would move it up by two. The new state is also moved horizontally by one enabling it to move toward the end of the track. Once the Agent\u2019s actions have been applied the wind value an additional action is then applied to the helicopter. An example of a possible action is to increase the Agents location by 2 in the vertical plane.\f\n\n\nDomain\n#\n\n\nThe domain of application is state space environment in which our entity, the \u201chelicopter\u201d will try to self-drive to live as long as possible in the randomly generated path. The environment is created as a matrix of variable size in both its coordinates of length and width. The entity's task is to complete the matrix starting from the left side and tries to travel successfully to the right side without interruption represented by the randomly generated obstacles.\n\n\nWe refer to this task as the example of a helicopter that has to maintain certain minimum and maximum quotes corresponding to the generate obstacles. To add small complexities, a wind model has been produced and also the Agent will be only given a partially observable state. To deal with the long-term deficiency of not receiving any reward during flight, the model receives a small reward as it navigates to the end of the window.\n\n\nState Space (Partially Observable):\n#\n\n\nThe reasoning behind why we chose a partially observable state space, a field of view, is that in real world situations it\u2019s rare that the full state of the system can be provided to the agent or even determined. A real-life example is equivocal to a pilot in a plane where the pilot is equipped with a radar such that he can increase his field of view, in turn, enabling a greater oversight of his current situation. For this reasoning for our model and world, it was only provided with a small field of view \u2013 as shown by the grid space on the right-hand side of the helicopter in the diagram below.\n\n\nPrevious reports, Minh et al., 2015 and Minh et al., 2013, considered the observable whole state space and used 4-10 frames as input into the Network. It is important to note here that this method attempts to embed knowledge into the Network, such that it can consider previous frames in addition to the current frame in order to determine Agents next action. A similar approach is used in our simulation were the model has access to the current and previous state for each training and prediction.\n\n\nplaceholder: field of view diagram\n\n\nModels Implemented:\n#\n\n\n\n\ne-Greedy Q Learning\n\n\ne-Greedy Q-Learning with epsilon decay\n\n\nDeep Q-Network (\nDQN\n)\n\n\n\n\nOther_Remarks\n#\n\n\nThis project differs was prepared for a University Course, which required the investigation to look at different Cases to understand the impact of changing the values for each parameter. The different Cases and selected can be found in the \nSettings.py\n file, selecting the Model selection can also be found in that file - the file is used in both the Training and Testing scripts.\n\n\n\n\nCase 1: Create task that a Reinforcement Learning Alogorithm could solve.\n\n\nCase 2: repeating the experiment in Case 1 with different gamma values\n\n\nCase 3: repeating the experiment in Case 1 with different learning rates\n\n\nCase 4: repeating the experiment in Case 1 with different policies\n\n\nCase 5: repeating the experiment in Case 1 with different state and reward functions", 
            "title": "Detailed Overview"
        }, 
        {
            "location": "/project_detail/#helicopter_game", 
            "text": "The purpose of this analysis is to provide the reader a concrete example of a classical Reinforcement Learning application.  The intention is to create a Reinforcement Learning algorithm to learn to play and complete the track, similar to the  Helicopter Game . The domain of implementation will be the contest of a randomly generated environment according to a particular function, and the learning task will provide the Agent a set of information enabling him to survive in the current environment for a longer duration of time.  The environment will provide to the Agent the possibility to move in five possible actions (directions), and the survival function will be defined as that function allowing the Agent to avoid obstacles generated in the environment.  Proposed algorithms for this task are all online policy iterations models with e-greedy updates policies: Q-Learning, Q-Learning with Epsilon Decay, and Deep Q-Learning Networks (DQN). A comparison of different parameters will be performed and evaluated.", 
            "title": "Helicopter_Game"
        }, 
        {
            "location": "/project_detail/#representation", 
            "text": "The helicopter task can be summarized as a survival task - the longer it travels along the path, the better its performance. However, the maximum reward is gained only by reaching the end of the track. The States following are represented by the different gradient colours in the part of the track which is not an obstacle, and the wind function guarantees a state which ranges from 0 to 6 according to which the helicopter is taking an action. Obstacles are initially set to be -10 and final reward to 100. The representation of the best route is the one by which the helicopter navigates the wind effectively and reaches the end of the track without any interruptions.  There are a number of possible actions it can take are either up, down, or continue laterally; different index increments dictate different actions. This index can range from 1 to 5, where an index of 1 would attempt to move the helicopter down by two, and an index of 5 would move it up by two. The new state is also moved horizontally by one enabling it to move toward the end of the track. Once the Agent\u2019s actions have been applied the wind value an additional action is then applied to the helicopter. An example of a possible action is to increase the Agents location by 2 in the vertical plane.", 
            "title": "Representation"
        }, 
        {
            "location": "/project_detail/#domain", 
            "text": "The domain of application is state space environment in which our entity, the \u201chelicopter\u201d will try to self-drive to live as long as possible in the randomly generated path. The environment is created as a matrix of variable size in both its coordinates of length and width. The entity's task is to complete the matrix starting from the left side and tries to travel successfully to the right side without interruption represented by the randomly generated obstacles.  We refer to this task as the example of a helicopter that has to maintain certain minimum and maximum quotes corresponding to the generate obstacles. To add small complexities, a wind model has been produced and also the Agent will be only given a partially observable state. To deal with the long-term deficiency of not receiving any reward during flight, the model receives a small reward as it navigates to the end of the window.", 
            "title": "Domain"
        }, 
        {
            "location": "/project_detail/#state-space-partially-observable", 
            "text": "The reasoning behind why we chose a partially observable state space, a field of view, is that in real world situations it\u2019s rare that the full state of the system can be provided to the agent or even determined. A real-life example is equivocal to a pilot in a plane where the pilot is equipped with a radar such that he can increase his field of view, in turn, enabling a greater oversight of his current situation. For this reasoning for our model and world, it was only provided with a small field of view \u2013 as shown by the grid space on the right-hand side of the helicopter in the diagram below.  Previous reports, Minh et al., 2015 and Minh et al., 2013, considered the observable whole state space and used 4-10 frames as input into the Network. It is important to note here that this method attempts to embed knowledge into the Network, such that it can consider previous frames in addition to the current frame in order to determine Agents next action. A similar approach is used in our simulation were the model has access to the current and previous state for each training and prediction.  placeholder: field of view diagram", 
            "title": "State Space (Partially Observable):"
        }, 
        {
            "location": "/project_detail/#models-implemented", 
            "text": "e-Greedy Q Learning  e-Greedy Q-Learning with epsilon decay  Deep Q-Network ( DQN )", 
            "title": "Models Implemented:"
        }, 
        {
            "location": "/project_detail/#other_remarks", 
            "text": "This project differs was prepared for a University Course, which required the investigation to look at different Cases to understand the impact of changing the values for each parameter. The different Cases and selected can be found in the  Settings.py  file, selecting the Model selection can also be found in that file - the file is used in both the Training and Testing scripts.   Case 1: Create task that a Reinforcement Learning Alogorithm could solve.  Case 2: repeating the experiment in Case 1 with different gamma values  Case 3: repeating the experiment in Case 1 with different learning rates  Case 4: repeating the experiment in Case 1 with different policies  Case 5: repeating the experiment in Case 1 with different state and reward functions", 
            "title": "Other_Remarks"
        }, 
        {
            "location": "/implementation/", 
            "text": "Track Generation:\n#\n\n\nFor this project we developed a couple of scripts to create tracks, the tracks comprise of some obstacles that can either protrude from the ceiling of the track or upwards from the floor. To implement this, we first generate a random number of tuples which contain four attributes; width and height of the obstacles, starting location in the window and if it is on the ceiling or floor of the window. The set of tuples that are generated can then be made into windows, where a window will contain one obstacle.\n\n\nTo create the track, a random number of windows are then selected, trimmed and concatenated to make a track. At the window generation phase, the window is initially created with zeros and ones where ones are the obstacle. We then fill the zeros in with a function (\nx**2 + 2 * y**2\n) to generate a continuous value, which are then be binned into seven distinct bins which correspond to different actions of the wind.\n\n\nThere are two files that can be used to generate tracks - with and without wind is possible.\n\n\n/Model/Build_tracks.py\n/Model/Build_tracks_wind.py\n\n\n\nThese python scripts simply calling the classes contained within the following files to get the respective tracks.\n\n\n/Model/Generate_obstacles.py\n/Model/Wind_Generation.py\n\n\n\nAgent\n#\n\n\n/Model/Agent.py\n\n\n\nThis file contains the mapping for action to location update with respect to the world. There are two functions:\n\n\n\n\nAction Controller : this controls the behaviour of the Agent that it has the ability to change\n\n\nWind Controller : dependinng on the location (x, y) that the Agent is currently at dictates the resulting action of the Agent. The Q-values that are calculate are independent of this Controller.\n\n\n\n\nQ-Learning Classes\n#\n\n\n/Model/Q_Learning_Agent.py\n\n\n\nWithin this file there are 3 classes - one per type of model. At the heart of all three classes is there Q-Learning function.\n\n\ndef learnQ(self, state, action, reward, value):\n\n    old_value = self.q.get((state, action), None)\n\n    if old_value is None:\n        self.q[(state, action)] = reward\n\n    else:\n        self.q[(state, action)] = old_value + \\\n            self.alpha * (value - old_value)\n\n\n\nThe three models are:\n\n\n1. e-Greedy Q Learning\n2. e-Greedy Q-Learning with epsilon decay\n3. Deep Q-Network (*DQN*)\n\n\n\nModels 1-2\n#\n\n\nBoth models make use of a python dictionary in order to map State-Action pairs to rewards. By using this method as opposed to a pre-computed table is that there was no startup cost associated with calculating them inadvance of training the model.\n\n\nThe primary disadvantage is that if the model was used in a predictive state and the Agent had not seen a particular pair before then it's performance with regards to the optimal policy be terrible. It would therefore be deemed that the model is generalising poorly - it would be hoped that in the finite state space that the model is learning that this would not be the case.\n\n\nModel 3\n#\n\n\nModel 3 uses a Neural Network to Approximate the\n\n\nChallenges and Issues Resolved\n#\n\n\nTODO", 
            "title": "Implementation"
        }, 
        {
            "location": "/implementation/#track-generation", 
            "text": "For this project we developed a couple of scripts to create tracks, the tracks comprise of some obstacles that can either protrude from the ceiling of the track or upwards from the floor. To implement this, we first generate a random number of tuples which contain four attributes; width and height of the obstacles, starting location in the window and if it is on the ceiling or floor of the window. The set of tuples that are generated can then be made into windows, where a window will contain one obstacle.  To create the track, a random number of windows are then selected, trimmed and concatenated to make a track. At the window generation phase, the window is initially created with zeros and ones where ones are the obstacle. We then fill the zeros in with a function ( x**2 + 2 * y**2 ) to generate a continuous value, which are then be binned into seven distinct bins which correspond to different actions of the wind.  There are two files that can be used to generate tracks - with and without wind is possible.  /Model/Build_tracks.py\n/Model/Build_tracks_wind.py  These python scripts simply calling the classes contained within the following files to get the respective tracks.  /Model/Generate_obstacles.py\n/Model/Wind_Generation.py", 
            "title": "Track Generation:"
        }, 
        {
            "location": "/implementation/#agent", 
            "text": "/Model/Agent.py  This file contains the mapping for action to location update with respect to the world. There are two functions:   Action Controller : this controls the behaviour of the Agent that it has the ability to change  Wind Controller : dependinng on the location (x, y) that the Agent is currently at dictates the resulting action of the Agent. The Q-values that are calculate are independent of this Controller.", 
            "title": "Agent"
        }, 
        {
            "location": "/implementation/#q-learning-classes", 
            "text": "/Model/Q_Learning_Agent.py  Within this file there are 3 classes - one per type of model. At the heart of all three classes is there Q-Learning function.  def learnQ(self, state, action, reward, value):\n\n    old_value = self.q.get((state, action), None)\n\n    if old_value is None:\n        self.q[(state, action)] = reward\n\n    else:\n        self.q[(state, action)] = old_value + \\\n            self.alpha * (value - old_value)  The three models are:  1. e-Greedy Q Learning\n2. e-Greedy Q-Learning with epsilon decay\n3. Deep Q-Network (*DQN*)", 
            "title": "Q-Learning Classes"
        }, 
        {
            "location": "/implementation/#models-1-2", 
            "text": "Both models make use of a python dictionary in order to map State-Action pairs to rewards. By using this method as opposed to a pre-computed table is that there was no startup cost associated with calculating them inadvance of training the model.  The primary disadvantage is that if the model was used in a predictive state and the Agent had not seen a particular pair before then it's performance with regards to the optimal policy be terrible. It would therefore be deemed that the model is generalising poorly - it would be hoped that in the finite state space that the model is learning that this would not be the case.", 
            "title": "Models 1-2"
        }, 
        {
            "location": "/implementation/#model-3", 
            "text": "Model 3 uses a Neural Network to Approximate the", 
            "title": "Model 3"
        }, 
        {
            "location": "/implementation/#challenges-and-issues-resolved", 
            "text": "TODO", 
            "title": "Challenges and Issues Resolved"
        }, 
        {
            "location": "/q-learning/", 
            "text": "Models Implemented:\n#\n\n\n\n\ne-Greedy Q Learning (1)\n\n\ne-Greedy Q-Learning with e-decay (2)\n\n\nDeep Q-Network (\nDQN\n)(3)\n\n\n\n\nOverview of Q-Learning\n#\n\n\nAll the models discussed in the coursework will use an online update policy strategy \u2013 an epsilon-greedy which intends to ensure adequate exploration of all the state space. Rummery and Niranjan (Rmmery et al., 1994) [9] provides an example of setting in which similar procedure are adopted. To exploit differences in the result of this self-driving helicopter simulation, we will compare three methods in which a different policy has been applied to select the best actions.\n\n\nQ-learning uses temporal differences to estimate the value of Q\n(s,a). In Q-learning, the agent maintains a table of Q[S,A], where S is the set of states and A is the set of actions. Q[s,a] represents its current estimate of Q\n(s,a).\n\n\nThe Q-Learning variables are represented below:\n\n\n\n\nQ(s, a) = Q value of a given state and action\n\n\na = Action\n\n\ns = State\n\n\nr = Reward\n\n\nR = Maximum reward for action a\n\n\n\n\nWhere Q-Learning can be represented by:\n\n\nQ[s,a] \u2190Q[s,a] + \u03b1(r+ \u03b3maxa' Q[s',a'] - Q[s,a])\n\n\n\nMotivation for DQN\n#\n\n\nLast year, Deep Q Networks (DQN) were brought to the attention of many researchers when Deepmind released a paper demonstrating the network's capability at playing Atari games.\n\n\nThe research featured in the Nature publication and showed that their implementation had overcome the issues that had typically been challenged when using a Neural Network as a function approximation for the Q values. Summarized in the table below, from the paper Playing Atari with Deep Reinforcement Learning (Deepmind, 2015) [16], the issue is discussed as well as the techniques that have been used to overcome these problems.\n\n\n\n\n\n\n\n\nIssues\n\n\nTechniques\n\n\n\n\n\n\n\n\n\n\nStability Issues\n\n\nReward Clipping\n\n\n\n\n\n\nDistribution of the data can change quickly\n\n\nError Clipping (Truncation)\n\n\n\n\n\n\n\n\nThe issues outlined in the table were implemented and were shown to have a real impact on the capability of the approximation. The analysis indicated that normalizing the range of the reward to a finite range helped to support the issue of dealing with large Q-values and their respective gradients \u2013 one negative of this approach was that the model may find it harder to differentiate the difference between small and large rewards due to the normalization. The second technique that was introduced was error clipping into the model \u2013 this is a frequently used method to deal with the potential of exploding gradients. The pseudocode below is a high-level description of the methods that were implemented in the Deepmind paper and in our model.\n\n\nImplementing the DQN\n#\n\n\nThe sequential nature of our problem means that it could potentially benefit from using a Recurrent Neural Network either as primary function approximator or as additional layers in the network, as in Mnih (Mnih et al., 2015) [15]. The original paper described that a Convolutional Neural Network (CNN) was used to \u201cwatch\u201d the replays of the game. In our implementation of the Deep Q-Network, we used a CNN that outputs to a Long Short-Term Memory (LSTM) layer and then finally into a linear output layer providing the Q-Values from the model. The key distinction to the original paper was that an LSTM layer was used where it has been demonstrated in many papers previously that an RNN is accomplished at capturing temporal patterns in sequential data. Since the goal of Q-Learning is to learn good policies for sequential decision making it, therefore, seemed appropriate to include this layer type.\n\n\nThe implementation initially developed made use of similar/simpler architecture to that proposed in previous work. This architecture that was implemented consisted of an Embedding Layer, which mapped the integer values (state) to a one-dimensional vector which were then passed into a Convolutional layer, followed by a Max-Pooling layer and lastly into two Dense layers (one with a Relu activation and the last a Softmax function). The intention was to replace the Dense layers with Recurrent layers, however, throughout testing, it was found that stability and the time to experiment with varying factors meant that the desire to move to the potentially enhanced version could not be achieved within the time frame. One feature that was implemented was that the model could record and store instances of the transitions and experience replay. This idea was used in one the most successful use cases of Neural Networks in Reinforcement Learning; this model was called TD-Gammon.\n\n\nThe model was developed in Keras, a Deep Learning library for Python, the primary reason for this is that it provides a very easy layer of abstraction on top of Theano or Tensorflow.\n\n\nThe diagram below captures the architecture used for developing the model.\n\n\nplaceholder: field of view diagram\n\n\nExpectations\n#\n\n\nFor both all of the Q-Learning variants it is expected that:\n\n\n\n\nthe model will probably go back in forth between preferring different actions over others during the initialisation\n\n\nfinding an appropriate learning rate and Gammma value for will be crucial\n\n\nensuring that the terminal state can be reached otherwise not enough learning can be stimulated\n\n\n\n\n\n\nDQN learning models will be sufficiently more difficult to training\n\n\ntuning of both the model and q-learning parameters will be critical to ensure the model converges", 
            "title": "Q-Learning Models"
        }, 
        {
            "location": "/q-learning/#models-implemented", 
            "text": "e-Greedy Q Learning (1)  e-Greedy Q-Learning with e-decay (2)  Deep Q-Network ( DQN )(3)", 
            "title": "Models Implemented:"
        }, 
        {
            "location": "/q-learning/#overview-of-q-learning", 
            "text": "All the models discussed in the coursework will use an online update policy strategy \u2013 an epsilon-greedy which intends to ensure adequate exploration of all the state space. Rummery and Niranjan (Rmmery et al., 1994) [9] provides an example of setting in which similar procedure are adopted. To exploit differences in the result of this self-driving helicopter simulation, we will compare three methods in which a different policy has been applied to select the best actions.  Q-learning uses temporal differences to estimate the value of Q (s,a). In Q-learning, the agent maintains a table of Q[S,A], where S is the set of states and A is the set of actions. Q[s,a] represents its current estimate of Q (s,a).  The Q-Learning variables are represented below:   Q(s, a) = Q value of a given state and action  a = Action  s = State  r = Reward  R = Maximum reward for action a   Where Q-Learning can be represented by:  Q[s,a] \u2190Q[s,a] + \u03b1(r+ \u03b3maxa' Q[s',a'] - Q[s,a])", 
            "title": "Overview of Q-Learning"
        }, 
        {
            "location": "/q-learning/#motivation-for-dqn", 
            "text": "Last year, Deep Q Networks (DQN) were brought to the attention of many researchers when Deepmind released a paper demonstrating the network's capability at playing Atari games.  The research featured in the Nature publication and showed that their implementation had overcome the issues that had typically been challenged when using a Neural Network as a function approximation for the Q values. Summarized in the table below, from the paper Playing Atari with Deep Reinforcement Learning (Deepmind, 2015) [16], the issue is discussed as well as the techniques that have been used to overcome these problems.     Issues  Techniques      Stability Issues  Reward Clipping    Distribution of the data can change quickly  Error Clipping (Truncation)     The issues outlined in the table were implemented and were shown to have a real impact on the capability of the approximation. The analysis indicated that normalizing the range of the reward to a finite range helped to support the issue of dealing with large Q-values and their respective gradients \u2013 one negative of this approach was that the model may find it harder to differentiate the difference between small and large rewards due to the normalization. The second technique that was introduced was error clipping into the model \u2013 this is a frequently used method to deal with the potential of exploding gradients. The pseudocode below is a high-level description of the methods that were implemented in the Deepmind paper and in our model.", 
            "title": "Motivation for DQN"
        }, 
        {
            "location": "/q-learning/#implementing-the-dqn", 
            "text": "The sequential nature of our problem means that it could potentially benefit from using a Recurrent Neural Network either as primary function approximator or as additional layers in the network, as in Mnih (Mnih et al., 2015) [15]. The original paper described that a Convolutional Neural Network (CNN) was used to \u201cwatch\u201d the replays of the game. In our implementation of the Deep Q-Network, we used a CNN that outputs to a Long Short-Term Memory (LSTM) layer and then finally into a linear output layer providing the Q-Values from the model. The key distinction to the original paper was that an LSTM layer was used where it has been demonstrated in many papers previously that an RNN is accomplished at capturing temporal patterns in sequential data. Since the goal of Q-Learning is to learn good policies for sequential decision making it, therefore, seemed appropriate to include this layer type.  The implementation initially developed made use of similar/simpler architecture to that proposed in previous work. This architecture that was implemented consisted of an Embedding Layer, which mapped the integer values (state) to a one-dimensional vector which were then passed into a Convolutional layer, followed by a Max-Pooling layer and lastly into two Dense layers (one with a Relu activation and the last a Softmax function). The intention was to replace the Dense layers with Recurrent layers, however, throughout testing, it was found that stability and the time to experiment with varying factors meant that the desire to move to the potentially enhanced version could not be achieved within the time frame. One feature that was implemented was that the model could record and store instances of the transitions and experience replay. This idea was used in one the most successful use cases of Neural Networks in Reinforcement Learning; this model was called TD-Gammon.  The model was developed in Keras, a Deep Learning library for Python, the primary reason for this is that it provides a very easy layer of abstraction on top of Theano or Tensorflow.  The diagram below captures the architecture used for developing the model.  placeholder: field of view diagram", 
            "title": "Implementing the DQN"
        }, 
        {
            "location": "/q-learning/#expectations", 
            "text": "For both all of the Q-Learning variants it is expected that:   the model will probably go back in forth between preferring different actions over others during the initialisation  finding an appropriate learning rate and Gammma value for will be crucial  ensuring that the terminal state can be reached otherwise not enough learning can be stimulated    DQN learning models will be sufficiently more difficult to training  tuning of both the model and q-learning parameters will be critical to ensure the model converges", 
            "title": "Expectations"
        }, 
        {
            "location": "/release_notes/", 
            "text": "To Do List:\n#\n\n\nMKDocs Documentation:\n\n\n\n\nIndex - DONE\n\n\nImplementation\n\n\nDISCUSS CHALLENGES WITH THE IMPLEMENTATION AND HOW THEY WERE OVERCOME\n\n\n\n\n\n\nProject Detail - DONE\n\n\nQ-Learning - DONE\n\n\nReferences - DONE\n\n\nVideo\n\n\nADD VIDEO", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release_notes/#to-do-list", 
            "text": "MKDocs Documentation:   Index - DONE  Implementation  DISCUSS CHALLENGES WITH THE IMPLEMENTATION AND HOW THEY WERE OVERCOME    Project Detail - DONE  Q-Learning - DONE  References - DONE  Video  ADD VIDEO", 
            "title": "To Do List:"
        }, 
        {
            "location": "/videos/", 
            "text": "Demonstrations\n#\n\n\nThe intention is to demo all three models:\n\n\n\n\nsimple Q-Learning Model\n2   simple Q-Learning Model with Epsilon Decay\n\n\nDeep Q-Learning (Neural Network)\n\n\n\n\nThe videos are currently being created...", 
            "title": "Video Demo"
        }, 
        {
            "location": "/videos/#demonstrations", 
            "text": "The intention is to demo all three models:   simple Q-Learning Model\n2   simple Q-Learning Model with Epsilon Decay  Deep Q-Learning (Neural Network)   The videos are currently being created...", 
            "title": "Demonstrations"
        }, 
        {
            "location": "/references/", 
            "text": "References:\n#\n\n\n\n\n\n\nSutton, R.S., Barto, A.G.: Reinforcement  Learning: An Introduction. MIT Press, Cambridge, MA (1998)\n\n\n\n\n\n\nWatkins, C.: Learning from Delayed Rewards. Ph.D. thesis, University of Cambridge, Cambridge, England (1989)\n\n\n\n\n\n\nThrun, S.B.: an Efficient Exploration in Reinforcement Learning. Technical Report CMU-CS-92-102, Carnegie Mellon University, Pittsburgh, PA, USA (1992)\n\n\n\n\n\n\nBrafman, R.I., Tennenholtz, M.: R-MAX - a general polynomial time algorithm for near-optimal Reinforcement learning. Journal of Machine Learning Research 3 (2002) 213\u2013231\n\n\n\n\n\n\nIshii, S., Yoshida, W., Yoshimoto, J.: Control of exploitation-exploration meta parameter in Reinforcement learning. Neural Networks 15(4-6) (2002) 665\u2013687\n\n\n\n\n\n\nHeidrich-Meisner, V.: Interview with Richard S. Sutton. K\u00a8unstliche Intelligenz 3 (2009) 41\u201343\n\n\n\n\n\n\nVermorel, J., Mohri, M.: Multi-armed bandit algorithms and empirical evaluation. In: Proceedings of the 16th European Conference on Machine Learning (ECML\u201905), Porto, Portugal (2005) 437\u2013448\n\n\n\n\n\n\nCaelen, O., Bontempi, G.: Improving the exploration strategy in bandit algorithms. In: Learning and Intelligent Optimization. Number 5313 in LNCS. Springer (2008) 56\u201368\n\n\n\n\n\n\nRummery, G.A., Niranjan, M.: On-line Q-learning using connectionist systems. Technical Report CUED/F-\nINFENG/TR 166, Cambridge University (1994)\n\n\n\n\n\n\nBertsekas, D.P.: Dynamic Programming: Deterministic and Stochastic Models. Prentice Hall (1987)\n\n\n\n\n\n\nGeorge, A.P., Powell, W.B.: Adaptive step sizes for recursive estimation with applications in approximate dynamic programming. Machine Learning 65(1) (2006) 167\u2013198\n\n\n\n\n\n\nRobbins, H.: Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society 58 (1952) 527\u2013535\n\n\n\n\n\n\nAwerbuch, B., Kleinberg, R.D.: Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In: Proceedings of the 36th Annual ACM Symposium on Theory of Computing, Chicago, IL, USA, ACM (2004) 45\u201353\n\n\n\n\n\n\nAzoulay-Schwartz, R., Kraus, S., Wilkenfeld, J.: Exploitation vs. exploration: Choosing a supplier in an environment of incomplete information. Decision Support Systems 38(1) (2004) 1\u201318\n\n\n\n\n\n\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. \u2018Human-Level Control through Deep Reinforcement  Learning\u2019. Nature 518, no. 7540 (25 February 2015): 529\u201333. doi:10.1038/nature14236\n\n\n\n\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., 2013. Playing Atari with Deep Reinforcement  Learning. arXiv preprint arXiv:1312.5602.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "Sutton, R.S., Barto, A.G.: Reinforcement  Learning: An Introduction. MIT Press, Cambridge, MA (1998)    Watkins, C.: Learning from Delayed Rewards. Ph.D. thesis, University of Cambridge, Cambridge, England (1989)    Thrun, S.B.: an Efficient Exploration in Reinforcement Learning. Technical Report CMU-CS-92-102, Carnegie Mellon University, Pittsburgh, PA, USA (1992)    Brafman, R.I., Tennenholtz, M.: R-MAX - a general polynomial time algorithm for near-optimal Reinforcement learning. Journal of Machine Learning Research 3 (2002) 213\u2013231    Ishii, S., Yoshida, W., Yoshimoto, J.: Control of exploitation-exploration meta parameter in Reinforcement learning. Neural Networks 15(4-6) (2002) 665\u2013687    Heidrich-Meisner, V.: Interview with Richard S. Sutton. K\u00a8unstliche Intelligenz 3 (2009) 41\u201343    Vermorel, J., Mohri, M.: Multi-armed bandit algorithms and empirical evaluation. In: Proceedings of the 16th European Conference on Machine Learning (ECML\u201905), Porto, Portugal (2005) 437\u2013448    Caelen, O., Bontempi, G.: Improving the exploration strategy in bandit algorithms. In: Learning and Intelligent Optimization. Number 5313 in LNCS. Springer (2008) 56\u201368    Rummery, G.A., Niranjan, M.: On-line Q-learning using connectionist systems. Technical Report CUED/F-\nINFENG/TR 166, Cambridge University (1994)    Bertsekas, D.P.: Dynamic Programming: Deterministic and Stochastic Models. Prentice Hall (1987)    George, A.P., Powell, W.B.: Adaptive step sizes for recursive estimation with applications in approximate dynamic programming. Machine Learning 65(1) (2006) 167\u2013198    Robbins, H.: Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society 58 (1952) 527\u2013535    Awerbuch, B., Kleinberg, R.D.: Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In: Proceedings of the 36th Annual ACM Symposium on Theory of Computing, Chicago, IL, USA, ACM (2004) 45\u201353    Azoulay-Schwartz, R., Kraus, S., Wilkenfeld, J.: Exploitation vs. exploration: Choosing a supplier in an environment of incomplete information. Decision Support Systems 38(1) (2004) 1\u201318    Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. \u2018Human-Level Control through Deep Reinforcement  Learning\u2019. Nature 518, no. 7540 (25 February 2015): 529\u201333. doi:10.1038/nature14236    Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., 2013. Playing Atari with Deep Reinforcement  Learning. arXiv preprint arXiv:1312.5602.", 
            "title": "References:"
        }
    ]
}